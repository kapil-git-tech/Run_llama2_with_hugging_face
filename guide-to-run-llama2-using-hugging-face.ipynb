{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Beginner's Guide to run llama2 using hugging face\n\n### Running llama 2 on kaggle after getting the access from meta\n\n* Make sure you have necessary permissions to access the hugging face repository name. If not, follow the below steps:\n* Go to meta website & login/sign-up. Use the same email id/username to get permissions to use Llama2 via hugging face. It takes 1-2 days for permissions to be granted by meta team (generally takes few hours)\n\n### In this example we run pre-trained model \"meta-llama/Llama-2-7b-chat-hf\"","metadata":{}},{"cell_type":"markdown","source":"## Creating token to login to Hugging Face\n\n* To access private repositories or specific resources on the Hugging Face model hub, you'll need an authentication token. \n* This token verifies your identity and permissions. You can generate a token from your Hugging Face account settings. \n* Visit https://huggingface.co/settings/tokens while logged in to your Hugging Face account.\n* Click on \"New Token\" to generate a new token.Provide a name for the token (e.g., \"My Token for Hugging Face CLI\"). Click \"Create\" to generate the token.","metadata":{}},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:29:06.251800Z","iopub.execute_input":"2023-08-15T09:29:06.252658Z","iopub.status.idle":"2023-08-15T09:29:18.549092Z","shell.execute_reply.started":"2023-08-15T09:29:06.252603Z","shell.execute_reply":"2023-08-15T09:29:18.548038Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()\n\n#Pass the hugging face token below, when prompted","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:29:18.550973Z","iopub.execute_input":"2023-08-15T09:29:18.551257Z","iopub.status.idle":"2023-08-15T09:29:18.723844Z","shell.execute_reply.started":"2023-08-15T09:29:18.551226Z","shell.execute_reply":"2023-08-15T09:29:18.722591Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e12ca041874625b74c2535e0d2a302"}},"metadata":{}}]},{"cell_type":"markdown","source":"### To pass your authentication token using the use_auth_token parameter in the from_pretrained function, you need to include the token in the URL as follows:\n\n* model = AutoModelForSequenceClassification.from_pretrained(\"username/repository-name\", use_auth_token=\"your-token-here\")\n* Replace \"your-token-here\" with the actual authentication token you've generated from the Hugging Face website. By providing the token in this way, the Hugging Face library will use it to authenticate and grant you access to the specific model or repository.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, TrainingArguments, Trainer\n\nmodel_name = \"meta-llama/Llama-2-7b-chat-hf\"\nTokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:29:29.881428Z","iopub.execute_input":"2023-08-15T09:29:29.881807Z","iopub.status.idle":"2023-08-15T09:29:43.022167Z","shell.execute_reply.started":"2023-08-15T09:29:29.881776Z","shell.execute_reply":"2023-08-15T09:29:43.021167Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a651bf13e9549bd9b536d7618b8fb9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c95fe9f237af408d88e5355611a5c88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a991f9c4824bd9a76510bf166bd38c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e63777f6caa4993982d10468288cbe1"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:29:43.024106Z","iopub.execute_input":"2023-08-15T09:29:43.025152Z","iopub.status.idle":"2023-08-15T09:33:06.242748Z","shell.execute_reply.started":"2023-08-15T09:29:43.025115Z","shell.execute_reply":"2023-08-15T09:33:06.241769Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f622b0da602b43a2bed653e79776594e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c29d7d236ade496dad9cb21e951d3c0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1497510d93a142499168fbe143057725"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14b12238cfad44dfa5da408cd5b223a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2cd22fd088646c0b33fa88016d690ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2bb7edc520a4a6c8983f84d15c39c2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31155e7f2c544edb8e8606286bf5302f"}},"metadata":{}}]},{"cell_type":"markdown","source":"### This below \"input\" part of the code is related to tokenizing text using the Hugging Face Transformers library, likely using a pre-trained tokenizer for natural language processing tasks.\n\n* Input_text is a variable that holds a text string. What ever you want to input into the model.\n* Tokenizer refers to a tokenizer object from the Hugging Face Transformers library. A tokenizer is used to process text data and convert it into a format that machine learning models can understand. It typically divides the text into tokens, converts them into numerical values (IDs)\n* The encode method of the tokenizer is being used to process the Input_text. It takes the input text and converts it into a format that can be fed into a machine learning model.\n* The return_tensors parameter specifies the format in which the encoded data should be returned. In this case, 'pt' stands for PyTorch tensors.\n\n### In Summary, the code takes the Input_text, encodes it using a tokenizer, and returns the encoded text as PyTorch tensors. This encoded text can then be used as input for various natural language processing tasks, such as language generation, sentiment analysis, or text classification, using pre-trained models available in the Hugging Face Transformers library.","metadata":{}},{"cell_type":"code","source":"Input_text = \"My favourite movie is The Departed. Can you recommend me other similar movies i might like?\"\nInputs = Tokenizer.encode(Input_text, return_tensors = 'pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:33:31.983734Z","iopub.execute_input":"2023-08-15T09:33:31.984109Z","iopub.status.idle":"2023-08-15T09:33:32.015504Z","shell.execute_reply.started":"2023-08-15T09:33:31.984082Z","shell.execute_reply":"2023-08-15T09:33:32.014677Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### This below \"output\" part of the code is related to using a language generation model from the Hugging Face Transformers library to generate text based on a given input.\n\n> * Outputs is a variable that will hold the generated text sequences. It will store the text sequences that are generated by the model.\n> * model refers to a language generation model from the Hugging Face Transformers library.\n> * The generate method of the model is being used to generate text. This method takes various parameters to control the generation process.\n> * Inputs likely holds the encoded input text that you've previously prepared using a tokenizer. This input text serves as a seed or prompt for the language generation model.\n> * The max_length parameter specifies the maximum length of the generated text sequences.\n> * The num_return_sequences parameter specifies how many different text sequences you want the model to generate. \n> * The temperature parameter controls the randomness of the generation process. A higher value (e.g., 1.0) makes the generated text more random, while a lower value (e.g., 0.7) makes it more focused and deterministic.","metadata":{}},{"cell_type":"code","source":"# Generate text\noutputs = model.generate(Inputs, max_length=100, num_return_sequences=5, temperature=0.7)\n\n# Print generated text\nprint(\"Generated text:\")\nfor i, output in enumerate(outputs):\n    # Decode the generated output text\n    decoded_output = Tokenizer.decode(output)\n    \n    # Find the index where the decoded output text starts\n    start_index = decoded_output.find(Input_text) + len(Input_text)\n    \n    # Extract and print the generated text without the input/question\n    generated_text = decoded_output[start_index:]\n    print(f\"{i}: {generated_text}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:39:54.127947Z","iopub.execute_input":"2023-08-15T09:39:54.129016Z","iopub.status.idle":"2023-08-15T09:44:11.771862Z","shell.execute_reply.started":"2023-08-15T09:39:54.128978Z","shell.execute_reply":"2023-08-15T09:44:11.770939Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Generated text:\n0: \n\nI'm looking for something that has a similar mix of crime, drama and suspense as The Departed.\n\nThanks!</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n1: \n\nThere are a few reasons why I think The Departed is my favourite movie. Firstly, it has an all-star cast, including Leonardo DiCaprio, Matt Damon, Jack Nicholson and Mark Wahlberg. The story is also incredibly engaging, with plenty of twists and turns to keep you on the edge of your seat. Additionally,\n2: \n\nYour friend,\nJohn </s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n3: \nThe Departed is a crime drama directed by Martin Scorsese, released in 2006. It tells the story of an undercover cop (Leonardo DiCaprio) who infiltrates a Boston crime syndicate, while a mole within the police department (Matt Damon) works to uncover his identity. The movie explores themes of\n4: \nI have a few movies that are similar to The Departed that you might enjoy. Here are a few recommendations:\n1. Infernal Affairs (2002) - This Rewrite movie is also a crime drama and follows the story of an undercover cop who infiltrates a crime syndicate, just like in The Departed.\n2. Sc\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CHATGPT Response to the same prompt:\n\n### My favourite movie is The Departed. Can you recommend me other similar movies i might like?\n### Return only 5 sequences with max length = 500\n\n### For the above prompt on **ChatGPT**, i got the follwing response:\n\nCertainly! If you loved \"The Departed,\" you might also enjoy these:\n\nInfernal Affairs (2002) - Hong Kong thriller, undercover cops.\n\nHeat (1995) - Crime, detective vs. criminal.\n\nDonnie Brasco (1997) - FBI agent infiltrates mafia.\n\nThe Town (2010) - Bank robbers, moral conflicts.\n\nAmerican Gangster (2007) - Detective vs. drug lord.","metadata":{}}]}